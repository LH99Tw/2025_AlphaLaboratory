import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')
from datetime import datetime
import os
import gc
import json
from scipy.stats import skew, kurtosis
from math import sqrt

try:
    import pyarrow  # I/O ê°€ì†ìš© (ìˆìœ¼ë©´ pandasê°€ ìë™ ì‚¬ìš©)
    _HAS_ARROW = True
except Exception:
    _HAS_ARROW = False

from concurrent.futures import ProcessPoolExecutor, as_completed


class LongOnlyBacktestSystem:
    def __init__(self, price_file=None, alpha_file=None, config_file=None):
        if config_file is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            config_file = os.path.join(current_dir, 'backtest_config.json')
        if price_file is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            price_file = os.path.join(current_dir, '..', 'database', 'sp500_interpolated.csv')
        if alpha_file is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            alpha_file = os.path.join(current_dir, '..', 'database', 'sp500_with_alphas.csv')

        self.price_file = price_file
        self.alpha_file = alpha_file
        self.config_file = config_file
        self.results = {}
        self.config = self.load_config()

        if not os.path.exists(self.price_file):
            raise FileNotFoundError(f"ì£¼ê°€ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.price_file}")
        if not os.path.exists(self.alpha_file):
            raise FileNotFoundError(f"ì•ŒíŒŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.alpha_file}")

        # ë‚´ë¶€ ìºì‹œ
        self._base_df = None     # Date, Ticker, Close, NextDayReturn (+ ì„ íƒëœ factorë“¤)
        self._selected_factors = None

    def load_config(self):
        try:
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            print(f"âœ… ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {self.config_file}")
            # ì„±ëŠ¥ ì˜µì…˜ ê¸°ë³¸ê°’ ë³´ê°•
            config.setdefault("performance_toggles", {
                "load_in_memory": True,
                "use_parquet_cache": False,
                "parquet_dir": "parquet_cache",
                "compute_rolling_mdd": True,
                "parallel_factor_eval": False,
                "max_workers": max(1, os.cpu_count() - 1 if os.cpu_count() else 1)
            })
            return config
        except FileNotFoundError:
            print(f"âš ï¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.config_file}")
            print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            cfg = self.get_default_config()
            return cfg
        except json.JSONDecodeError as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ JSON ì˜¤ë¥˜: {e}")
            print("ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self.get_default_config()

    def get_default_config(self):
        return {
            "backtest_settings": {
                "start_date": "2013-01-01",
                "end_date": "2015-12-31",
                "max_factors": 101,
                "quantile": 0.1,
                "transaction_cost": 0.001,
                "chunk_size": 100000,
                "rebalancing_frequency": "daily"
            },
            "output_settings": {
                "output_directory": "calculated_alphas",
                "metrics_filename": "enhanced_backtest_metrics.csv",
                "factor_returns_prefix": "factor_returns_"
            },
            "performance_metrics": {
                "include_cagr": True,
                "include_sharpe": True,
                "include_sortino": True,
                "include_ic": True,
                "include_mdd": True,
                "include_winrate": True,
                "include_skewness": True,
                "include_kurtosis": True,
                "include_turnover": True
            },
            "performance_toggles": {
                "load_in_memory": True,
                "use_parquet_cache": False,
                "parquet_dir": "parquet_cache",
                "compute_rolling_mdd": True,
                "parallel_factor_eval": False,
                "max_workers": max(1, os.cpu_count() - 1 if os.cpu_count() else 1)
            }
        }

    def update_config(self, new_settings):
        self.config.update(new_settings)
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=4, ensure_ascii=False)
            print(f"âœ… ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {self.config_file}")
        except Exception as e:
            print(f"âŒ ì„¤ì • íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    def print_current_config(self):
        print("\nğŸ“‹ í˜„ì¬ ë°±í…ŒìŠ¤íŠ¸ ì„¤ì •:")
        print(json.dumps(self.config, indent=2, ensure_ascii=False))

    def get_alpha_columns(self):
        with open(self.alpha_file, 'r') as f:
            header = f.readline().strip().split(',')
            alpha_columns = [col for col in header if col.startswith('alpha')]
        return alpha_columns

    # =========================
    #   í•µì‹¬: í•œ ë²ˆë§Œ ë¡œë“œ/ë³‘í•©
    # =========================
    def _prepare_base_frame(self, start_date, end_date, selected_factors):
        """Date/Ticker ê¸°ì¤€ í•œ ë²ˆë§Œ ë³‘í•© + NextDayReturn 1íšŒ ê³„ì‚° (ì†ë„ í•µì‹¬)"""
        if self._base_df is not None and self._selected_factors == tuple(selected_factors):
            return self._base_df

        toggles = self.config.get("performance_toggles", {})
        use_cache = toggles.get("use_parquet_cache", False)
        parquet_dir = toggles.get("parquet_dir", "parquet_cache")
        os.makedirs(parquet_dir, exist_ok=True)

        read_kwargs = dict(engine="pyarrow") if _HAS_ARROW else {}
        dtypes = {"Ticker": "category"}

        # (ì„ íƒ) Parquet ìºì‹œ
        price_parquet = os.path.join(parquet_dir, "price.parquet")
        alpha_parquet = os.path.join(parquet_dir, "alphas.parquet")

        # ê°€ê²© ë¡œë“œ
        if use_cache and os.path.exists(price_parquet):
            price = pd.read_parquet(price_parquet, columns=["Date", "Ticker", "Close"])
        else:
            price = pd.read_csv(self.price_file, usecols=["Date", "Ticker", "Close"],
                                parse_dates=["Date"], dtype=dtypes, **read_kwargs)
            if use_cache:
                price.to_parquet(price_parquet, index=False)

        # ì•ŒíŒŒ ë¡œë“œ (ì„ íƒëœ íŒ©í„°ë§Œ)
        alpha_cols = ["Date", "Ticker"] + list(selected_factors)
        if use_cache and os.path.exists(alpha_parquet):
            alpha = pd.read_parquet(alpha_parquet, columns=alpha_cols)
        else:
            alpha = pd.read_csv(self.alpha_file, usecols=alpha_cols,
                                parse_dates=["Date"], dtype=dtypes, **read_kwargs)
            if use_cache:
                # ì „ì²´ë¥¼ ìºì‹±í•˜ë©´ íŒŒì¼ì´ ì»¤ì§ˆ ìˆ˜ ìˆìœ¼ë‹ˆ, ìµœì´ˆ 1íšŒ ì „ì²´ ìºì‹œë¥¼ ê¶Œì¥
                # ì—¬ê¸°ì„œëŠ” ì„ íƒëœ ì»¬ëŸ¼ë§Œ ìºì‹œ
                alpha.to_parquet(alpha_parquet, index=False)

        # ê¸°ê°„ í•„í„°
        mask_price = (price["Date"] >= start_date) & (price["Date"] <= end_date)
        mask_alpha = (alpha["Date"] >= start_date) & (alpha["Date"] <= end_date)
        price = price.loc[mask_price]
        alpha = alpha.loc[mask_alpha]

        # ë³‘í•© (í•œ ë²ˆ)
        base = pd.merge(price.sort_values(["Ticker", "Date"]),
                        alpha.sort_values(["Ticker", "Date"]),
                        on=["Date", "Ticker"], how="inner")

        # NextDayReturn (í•œ ë²ˆ)
        base["NextDayReturn"] = base.groupby("Ticker", observed=True)["Close"].shift(-1) / base["Close"] - 1
        base = base.dropna(subset=["NextDayReturn"])

        self._base_df = base
        self._selected_factors = tuple(selected_factors)
        return base

    # =========================
    #   ë²¡í„°í™”ëœ íŒ©í„° ìˆ˜ìµë¥  ê³„ì‚°
    # =========================
    def calculate_factor_returns(self, df, factor_col, quantile=0.1, rebalancing_frequency='daily'):
        if not pd.api.types.is_datetime64_any_dtype(df['Date']):
            df = df.copy()
            df['Date'] = pd.to_datetime(df['Date'])

        if rebalancing_frequency == 'daily':
            # ìƒìœ„ q ë§ˆìŠ¤í¬ ë²¡í„° ìƒì„±
            ranks = df.groupby('Date', observed=True)[factor_col].rank(pct=True, ascending=False)
            mask = ranks <= quantile
            sel = df.loc[mask, ["Date", "NextDayReturn", "Ticker"]]
            agg = sel.groupby("Date", observed=True).agg(
                LongReturn=("NextDayReturn", "mean"),
                LongCount=("Ticker", "count")
            ).reset_index()
            agg["FactorReturn"] = agg["LongReturn"]
            return agg.to_dict("records")

        # ê¸°ê°„ ë¦¬ë°¸ëŸ°ì‹±(ì£¼/ì›”/ë¶„ê¸°)
        df_sorted = df.sort_values("Date").copy()
        if rebalancing_frequency == 'weekly':
            df_sorted['Period'] = df_sorted['Date'].dt.to_period('W')
        elif rebalancing_frequency == 'monthly':
            df_sorted['Period'] = df_sorted['Date'].dt.to_period('M')
        elif rebalancing_frequency == 'quarterly':
            df_sorted['Period'] = df_sorted['Date'].dt.to_period('Q')
        else:
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ë¦¬ë°¸ëŸ°ì‹± ì£¼ê¸°: {rebalancing_frequency}. ì¼ë³„ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            return self.calculate_factor_returns(df, factor_col, quantile, 'daily')

        # ê¸°ê°„ ì²«ë‚  ë°ì´í„°
        first_day = df_sorted.groupby(["Period", "Ticker"], observed=True, as_index=False).first()
        # í•´ë‹¹ ì²«ë‚ ì˜ íŒ©í„° ìˆœìœ„ë¡œ ì¢…ëª© ì„ íƒ
        ranks = first_day.groupby("Period", observed=True)[factor_col].rank(pct=True, ascending=False)
        first_day["Chosen"] = ranks <= quantile
        chosen = first_day[first_day["Chosen"]][["Period", "Ticker"]]

        # ê° (Period, Ticker)ì˜ ì‹œì‘/ë ê°€ê²©ìœ¼ë¡œ ê¸°ê°„ ìˆ˜ìµë¥  ë²¡í„° ê³„ì‚°
        per_first = df_sorted.groupby(["Period", "Ticker"], observed=True)["Close"].first()
        per_last = df_sorted.groupby(["Period", "Ticker"], observed=True)["Close"].last()
        period_ret = (per_last / per_first - 1).rename("PeriodReturn").reset_index()

        merged = pd.merge(chosen, period_ret, on=["Period", "Ticker"], how="inner")

        # ê¸°ê°„ë³„ í‰ê·  ìˆ˜ìµë¥ /í¸ì… ìˆ˜
        by_p = merged.groupby("Period", observed=True).agg(
            LongReturn=("PeriodReturn", "mean"),
            LongCount=("Ticker", "count")
        ).reset_index()

        # ê° ê¸°ê°„ì˜ ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ Dateë¡œ ì±„íƒ
        period_last_date = df_sorted.groupby("Period", observed=True)["Date"].max().rename("Date").reset_index()
        out = pd.merge(by_p, period_last_date, on="Period", how="left")[["Date", "LongReturn", "LongCount"]]
        out["FactorReturn"] = out["LongReturn"]
        return out.to_dict("records")

    # =========================
    #   ëˆ„ì /ë¡¤ë§ ì§€í‘œ (ê°€ì†)
    # =========================
    def calculate_cumulative_returns(self, factor_returns, transaction_cost=0.001):
        fr = factor_returns.copy()
        return_col = 'LongReturn' if 'LongReturn' in fr.columns else 'FactorReturn'
        net_col = f"{return_col}_Net"
        fr[net_col] = fr[return_col] - transaction_cost

        # ëˆ„ì  ìˆ˜ìµë¥ 
        fr['CumulativeReturn'] = (1.0 + fr[net_col]).cumprod() - 1.0

        # ë¡¤ë§ ì§€í‘œ (ê°€ëŠ¥í•œ ê³³ì€ ë²¡í„°í™”)
        fr = self.calculate_rolling_metrics(fr, net_col)
        return fr

    def calculate_rolling_metrics(self, factor_returns, return_col, window_days=252):
        fr = factor_returns.copy()
        x = fr[return_col]

        # Sharpe(ì—°í™˜ì‚°)
        rolling_mean = x.rolling(window_days, min_periods=window_days).mean()
        rolling_std = x.rolling(window_days, min_periods=window_days).std(ddof=0)
        fr['RollingSharpe'] = (rolling_mean / rolling_std) * sqrt(252)

        # Volatility(ì—°í™˜ì‚°)
        fr['RollingVolatility'] = rolling_std * sqrt(252)

        # CAGR(ì—°í™˜ì‚°) - log í•©ìœ¼ë¡œ ì•ˆì •í™”
        log1p = np.log1p(x)
        roll_logsum = log1p.rolling(window_days, min_periods=window_days).sum()
        fr['RollingCAGR'] = np.exp(roll_logsum * (252 / window_days)) - 1.0

        # Win Rate
        fr['RollingWinRate'] = x.rolling(window_days, min_periods=window_days).apply(lambda s: (s > 0).mean())

        # Rolling MDD (ì˜µì…˜)
        toggles = self.config.get("performance_toggles", {})
        if toggles.get("compute_rolling_mdd", True):
            fr['RollingMDD'] = fr['CumulativeReturn'].rolling(window_days, min_periods=window_days).apply(
                lambda s: self.calculate_mdd(s)
            )
        else:
            fr['RollingMDD'] = np.nan

        return fr

    def calculate_mdd(self, cumulative_returns):
        if len(cumulative_returns) == 0:
            return np.nan
        curve = 1.0 + cumulative_returns
        peak = np.maximum.accumulate(curve)
        dd = (curve - peak) / peak
        return dd.min()

    def calculate_performance_metrics(self, factor_returns, cumulative_returns):
        total_return = cumulative_returns['CumulativeReturn'].iloc[-1]
        daily_returns = factor_returns['LongReturn_Net'] if 'LongReturn_Net' in factor_returns.columns else factor_returns['LongReturn']
        negative_returns = daily_returns[daily_returns < 0]

        days = len(factor_returns)
        years = days / 252 if days else 0
        cagr = (1 + total_return) ** (1 / years) - 1 if years > 0 else 0

        sharpe = daily_returns.mean() / daily_returns.std() * np.sqrt(252) if daily_returns.std() > 0 else 0
        sortino = daily_returns.mean() / (negative_returns.std() * np.sqrt(252)) if negative_returns.std() > 0 else 0
        hit_ratio = (daily_returns > 0).mean()
        skewness = skew(daily_returns) if len(daily_returns) > 2 else 0
        kurt = kurtosis(daily_returns) if len(daily_returns) > 3 else 0

        turnover = self.calculate_turnover(factor_returns)

        cumulative_curve = 1 + cumulative_returns['CumulativeReturn']
        running_max = cumulative_curve.expanding().max()
        drawdown = (cumulative_curve - running_max) / running_max
        mdd = drawdown.min()

        return {
            'CAGR': cagr,
            'MDD': mdd,
            'SharpeRatio': sharpe,
            'SortinoRatio': sortino,
            'WinRate': hit_ratio,
            'TotalReturn': total_return,
            'Volatility': daily_returns.std() * np.sqrt(252),
            'MaxDrawdown': mdd,
            'Skewness': skewness,
            'Kurtosis': kurt,
            'Turnover': turnover,
            'TotalDays': days
        }

    def calculate_ic(self, df, factor_col):
        valid = df[['Date', factor_col, 'NextDayReturn']].dropna()
        if valid.empty:
            return np.nan
        # ì¼ìë³„ ë‹¨ìˆœ ìˆœìœ„ ICë„ ê°€ëŠ¥í•˜ì§€ë§Œ, ê¸°ì¡´ ì •ì˜ ìœ ì§€
        return valid.groupby('Date', observed=True).apply(
            lambda x: x[factor_col].corr(x['NextDayReturn'])
        ).mean()

    def calculate_turnover(self, factor_returns):
        # ê¸°ì¡´ ë™ì‘(ë³´ìˆ˜ì  ì¶”ì •)ì„ ìœ ì§€í•´ë‹¬ë¼ í–ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë‘”ë‹¤.
        if 'LongCount' in factor_returns.columns:
            avg_count = factor_returns['LongCount'].mean()
            return 2 * avg_count / avg_count if avg_count else np.nan
        return np.nan

    # =========================
    #   íŒ©í„° ì²˜ë¦¬ (I/O 1íšŒ + ë²¡í„°)
    # =========================
    def process_single_factor(self, factor_name, start_date, end_date, quantile=0.1, transaction_cost=0.001, rebalancing_frequency='daily'):
        print(f"íŒ©í„° {factor_name} ì²˜ë¦¬ ì¤‘... (ë¦¬ë°¸ëŸ°ì‹±: {rebalancing_frequency})")

        # ì¤€ë¹„ëœ base frame ì¬ì‚¬ìš©
        base = self._base_df
        if base is None or factor_name not in base.columns:
            raise RuntimeError("ë‚´ë¶€ ë² ì´ìŠ¤ í”„ë ˆì„ ì¤€ë¹„ê°€ ë˜ì§€ ì•Šì•˜ê±°ë‚˜, ì„ íƒëœ íŒ©í„° ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        df = base[["Date", "Ticker", "Close", "NextDayReturn", factor_name]].dropna(subset=[factor_name]).copy()

        factor_returns_list = self.calculate_factor_returns(df, factor_name, quantile, rebalancing_frequency)
        if not factor_returns_list:
            return None, None

        factor_returns_df = pd.DataFrame(factor_returns_list).sort_values('Date').reset_index(drop=True)
        cumulative_returns = self.calculate_cumulative_returns(factor_returns_df, transaction_cost)
        performance_metrics = self.calculate_performance_metrics(factor_returns_df, cumulative_returns)

        # IC
        ic = self.calculate_ic(df, factor_name)
        performance_metrics['IC'] = ic

        performance_metrics['Factor'] = factor_name
        performance_metrics['RebalancingFrequency'] = rebalancing_frequency

        return performance_metrics, cumulative_returns

    # =========================
    #   ì‹¤í–‰ ì§„ì…ì 
    # =========================
    def run_backtest(self, start_date=None, end_date=None, quantile=None, transaction_cost=None, max_factors=None, rebalancing_frequencies=None):
        settings = self.config['backtest_settings']

        start_date = pd.to_datetime(start_date or settings['start_date'])
        end_date = pd.to_datetime(end_date or settings['end_date'])
        quantile = quantile or settings['quantile']
        transaction_cost = transaction_cost or settings['transaction_cost']
        max_factors = max_factors or settings['max_factors']

        if rebalancing_frequencies is None:
            rebalancing_frequencies = [settings.get('rebalancing_frequency', 'daily')]

        print(f"ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ì„¤ì •:")
        print(f"   - ê¸°ê°„: {start_date.date()} ~ {end_date.date()}")
        print(f"   - ë¶„ìœ„ìˆ˜: {quantile}")
        print(f"   - ê±°ë˜ë¹„ìš©: {transaction_cost}")
        print(f"   - ìµœëŒ€ íŒ©í„° ìˆ˜: {max_factors}")
        print(f"   - ë¦¬ë°¸ëŸ°ì‹± ì£¼ê¸°: {rebalancing_frequencies}")

        alpha_columns = self.get_alpha_columns()
        if max_factors:
            alpha_columns = alpha_columns[:max_factors]

        # í•µì‹¬: í•œ ë²ˆë§Œ ì½ê³  í•œ ë²ˆë§Œ ë³‘í•© (ì„ íƒëœ íŒ©í„°ë§Œ)
        self._prepare_base_frame(start_date, end_date, alpha_columns)

        all_results = {}
        toggles = self.config.get("performance_toggles", {})
        parallel = toggles.get("parallel_factor_eval", False)
        max_workers = toggles.get("max_workers", 1)

        for rebalancing_frequency in rebalancing_frequencies:
            print(f"\nğŸ”„ {rebalancing_frequency.upper()} ë¦¬ë°¸ëŸ°ì‹± ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘...")

            all_performance_metrics = []
            top_factor_returns = {}

            if parallel and len(alpha_columns) > 1 and max_workers > 1:
                # ë³‘ë ¬: íŒ©í„°ë³„ ê³„ì‚°
                with ProcessPoolExecutor(max_workers=max_workers) as ex:
                    futures = {
                        ex.submit(_run_single_factor_job,
                                  self.price_file, self.alpha_file, self.config,
                                  self._base_df, factor, start_date, end_date,
                                  quantile, transaction_cost, rebalancing_frequency): factor
                        for factor in alpha_columns
                    }
                    idx = 0
                    for fut in as_completed(futures):
                        factor = futures[fut]
                        try:
                            pm, fr = fut.result()
                            if pm is not None:
                                all_performance_metrics.append(pm)
                                top_factor_returns[factor] = fr
                                idx += 1
                                print(f"[{idx}/{len(alpha_columns)}] {factor}: "
                                      f"CAGR={pm['CAGR']:.4f}, "
                                      f"Sharpe={pm['SharpeRatio']:.4f}, "
                                      f"Sortino={pm.get('SortinoRatio', 0):.4f}, "
                                      f"IC={pm.get('IC', 0):.4f}, "
                                      f"WinRate={pm.get('WinRate', 0):.4f}, "
                                      f"MDD={pm.get('MDD', 0):.4f}")
                        except Exception as e:
                            print(f"{factor} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            else:
                # ì§ë ¬: ê¸°ì¡´ ì¶œë ¥ ìˆœì„œ ìœ ì§€
                for i, factor in enumerate(alpha_columns):
                    try:
                        pm, fr = self.process_single_factor(
                            factor, start_date, end_date, quantile, transaction_cost, rebalancing_frequency
                        )
                        if pm is not None:
                            all_performance_metrics.append(pm)
                            top_factor_returns[factor] = fr
                            print(f"[{i+1}/{len(alpha_columns)}] {factor}: "
                                  f"CAGR={pm['CAGR']:.4f}, "
                                  f"Sharpe={pm['SharpeRatio']:.4f}, "
                                  f"Sortino={pm.get('SortinoRatio', 0):.4f}, "
                                  f"IC={pm.get('IC', 0):.4f}, "
                                  f"WinRate={pm.get('WinRate', 0):.4f}, "
                                  f"MDD={pm.get('MDD', 0):.4f}")
                    except Exception as e:
                        print(f"{factor} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        continue

            all_results[rebalancing_frequency] = {
                'performance_metrics': all_performance_metrics,
                'factor_returns': top_factor_returns
            }

            self.save_results(all_performance_metrics, top_factor_returns, rebalancing_frequency)

        return all_results

    # (save_results / generate_summary_report ëŠ” ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ë³µì‚¬)
    # ---- ì•„ë˜ ë‘ ë©”ì„œë“œëŠ” ë„¤ ì›ë³¸ì„ ê·¸ëŒ€ë¡œ ë‘¬ë„ ë¨. í•„ìš” ì‹œ ë™ì¼í•˜ê²Œ ë¶™ì—¬ë„£ê¸° ----

    def save_results(self, performance_metrics, top_factor_returns, rebalancing_frequency='daily'):
        if not performance_metrics:
            return
        output_settings = self.config['output_settings']
        base_output_dir = output_settings['output_directory']
        metrics_filename = output_settings['metrics_filename']
        factor_prefix = output_settings['factor_returns_prefix']
        output_dir = os.path.join(base_output_dir, f"{rebalancing_frequency}_rebalancing")
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"ğŸ“ {output_dir} í´ë” ìƒì„± ì™„ë£Œ")

        metrics_df = pd.DataFrame(performance_metrics)
        metrics_path = os.path.join(output_dir, metrics_filename)
        metrics_df.to_csv(metrics_path, index=False)
        print(f"âœ… ì„±ëŠ¥ ì§€í‘œ ì €ì¥ ì™„ë£Œ: {metrics_path}")

        saved_count = 0
        for factor_name, factor_returns in top_factor_returns.items():
            factor_metrics = next((m for m in performance_metrics if m['Factor'] == factor_name), None)
            if factor_metrics is not None:
                factor_returns_with_metrics = factor_returns.copy()
                factor_returns_with_metrics.attrs['performance_metrics'] = {
                    'CAGR': factor_metrics['CAGR'],
                    'SharpeRatio': factor_metrics['SharpeRatio'],
                    'SortinoRatio': factor_metrics.get('SortinoRatio', 0),
                    'IC': factor_metrics.get('IC', 0),
                    'WinRate': factor_metrics.get('WinRate', 0),
                    'MDD': factor_metrics.get('MDD', 0),
                    'Skewness': factor_metrics.get('Skewness', 0),
                    'Kurtosis': factor_metrics.get('Kurtosis', 0),
                    'Turnover': factor_metrics.get('Turnover', 0),
                    'TotalReturn': factor_metrics.get('TotalReturn', 0),
                    'Volatility': factor_metrics.get('Volatility', 0)
                }
                factor_path = os.path.join(output_dir, f'{factor_prefix}{factor_name}.csv')
                with open(factor_path, 'w', newline='') as f:
                    f.write(f"# Performance Metrics for {factor_name}\n")
                    f.write(f"# CAGR: {factor_metrics['CAGR']:.6f}\n")
                    f.write(f"# SharpeRatio: {factor_metrics['SharpeRatio']:.6f}\n")
                    f.write(f"# SortinoRatio: {factor_metrics.get('SortinoRatio', 0):.6f}\n")
                    f.write(f"# IC: {factor_metrics.get('IC', 0):.6f}\n")
                    f.write(f"# WinRate: {factor_metrics.get('WinRate', 0):.6f}\n")
                    f.write(f"# MDD: {factor_metrics.get('MDD', 0):.6f}\n")
                    f.write(f"# Skewness: {factor_metrics.get('Skewness', 0):.6f}\n")
                    f.write(f"# Kurtosis: {factor_metrics.get('Kurtosis', 0):.6f}\n")
                    f.write(f"# Turnover: {factor_metrics.get('Turnover', 0):.6f}\n")
                    f.write(f"# TotalReturn: {factor_metrics.get('TotalReturn', 0):.6f}\n")
                    f.write(f"# Volatility: {factor_metrics.get('Volatility', 0):.6f}\n")
                    f.write(f"# TotalDays: {factor_metrics.get('TotalDays', 0)}\n")
                    f.write("#\n")
                factor_returns.to_csv(factor_path, mode='a', index=False)
                saved_count += 1
        print(f"âœ… {saved_count}ê°œ íŒ©í„° ìˆ˜ìµë¥  ì €ì¥ ì™„ë£Œ (ì„±ëŠ¥ ì§€í‘œ í¬í•¨): {output_dir}/")

    def generate_summary_report(self, performance_metrics):
        if not performance_metrics:
            return
        metrics_df = pd.DataFrame(performance_metrics)
        rebalancing_frequency = (
            metrics_df['RebalancingFrequency'].iloc[0]
            if 'RebalancingFrequency' in metrics_df.columns else 'unknown'
        )

        print("\n" + "="*80)
        print(f"ğŸ“Š ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ({rebalancing_frequency.upper()} ë¦¬ë°¸ëŸ°ì‹±)")
        print("="*80)

        # ì „ì²´ í†µê³„
        print(f"\nğŸ“ˆ ì „ì²´ íŒ©í„° í†µê³„ ({len(metrics_df)}ê°œ íŒ©í„°):")
        print(f"   â€¢ í‰ê·  CAGR: {metrics_df['CAGR'].mean():.4f}")
        print(f"   â€¢ í‰ê·  Sharpe Ratio: {metrics_df['SharpeRatio'].mean():.4f}")
        print(f"   â€¢ í‰ê·  Sortino Ratio: {metrics_df.get('SortinoRatio', pd.Series([0])).mean():.4f}")
        print(f"   â€¢ í‰ê·  IC: {metrics_df['IC'].mean():.4f}")
        print(f"   â€¢ í‰ê·  Win Rate: {metrics_df['WinRate'].mean():.4f}")
        print(f"   â€¢ í‰ê·  MDD: {metrics_df['MDD'].mean():.4f}")
        print(f"   â€¢ í‰ê·  Skewness: {metrics_df.get('Skewness', pd.Series([0])).mean():.4f}")
        print(f"   â€¢ í‰ê·  Kurtosis: {metrics_df.get('Kurtosis', pd.Series([0])).mean():.4f}")
        print(f"   â€¢ í‰ê·  Turnover: {metrics_df.get('Turnover', pd.Series([0])).mean():.4f}")

        # ìƒìœ„ 10ê°œ (Sharpe ê¸°ì¤€)
        print(f"\nğŸ† ìƒìœ„ 10ê°œ íŒ©í„° (Sharpe Ratio ê¸°ì¤€):")
        top_10_sharpe = metrics_df.nlargest(10, 'SharpeRatio')
        display_columns = ['Factor', 'CAGR', 'SharpeRatio', 'SortinoRatio', 'IC', 'WinRate', 'MDD', 'Skewness', 'Kurtosis']
        available_columns = [col for col in display_columns if col in top_10_sharpe.columns]
        print(top_10_sharpe[available_columns].to_string(index=False, float_format='%.4f'))

        # ìƒìœ„ 10ê°œ (CAGR ê¸°ì¤€)
        print(f"\nğŸ“ˆ ìƒìœ„ 10ê°œ íŒ©í„° (CAGR ê¸°ì¤€):")
        top_10_cagr = metrics_df.nlargest(10, 'CAGR')
        print(top_10_cagr[available_columns].to_string(index=False, float_format='%.4f'))

        # ìƒìœ„ 10ê°œ (IC ê¸°ì¤€)
        print(f"\nğŸ¯ ìƒìœ„ 10ê°œ íŒ©í„° (IC ê¸°ì¤€):")
        top_10_ic = metrics_df.nlargest(10, 'IC')
        print(top_10_ic[available_columns].to_string(index=False, float_format='%.4f'))

        print("\n" + "="*80)


# -------------------------------
# ë³‘ë ¬ ì‹¤í–‰ìš© í—¬í¼ (í”„ë¡œì„¸ìŠ¤ í’€)
# -------------------------------
def _run_single_factor_job(price_file, alpha_file, config, base_df, factor,
                           start_date, end_date, quantile, transaction_cost, rebalancing_frequency):
    """
    í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ë™ì¼ ë¡œì§ì„ ì‹¤í–‰í•˜ê¸° ìœ„í•œ í—¬í¼.
    """
    sys = LongOnlyBacktestSystem(price_file=price_file, alpha_file=alpha_file, config_file=None)
    sys.config = config
    sys._base_df = base_df
    sys._selected_factors = tuple([factor])
    return sys.process_single_factor(
        factor, start_date, end_date, quantile, transaction_cost, rebalancing_frequency
    )


# -------------------------------
# ì‹¤í–‰ë¶€
# -------------------------------
if __name__ == '__main__':
    backtest = LongOnlyBacktestSystem()

    # í˜„ì¬ ì„¤ì • ì¶œë ¥
    backtest.print_current_config()

    # ì„¤ì • íŒŒì¼ì—ì„œ ë¦¬ë°¸ëŸ°ì‹± ì£¼ê¸° ê°€ì ¸ì˜¤ê¸°
    target_frequency = backtest.config['backtest_settings'].get('rebalancing_frequency', 'daily')

    print(f"\nğŸš€ {target_frequency.upper()} ë¦¬ë°¸ëŸ°ì‹±ìœ¼ë¡œ ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")

    # ë‹¨ì¼ ë¦¬ë°¸ëŸ°ì‹± ì£¼ê¸°ë¡œ ì‹¤í–‰
    all_results = backtest.run_backtest(rebalancing_frequencies=[target_frequency])

    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    for frequency, results in all_results.items():
        print("\n" + "="*80)
        print(f"ğŸ“Š {frequency.upper()} ë¦¬ë°¸ëŸ°ì‹± ê²°ê³¼ ìš”ì•½")
        print("="*80)
        backtest.generate_summary_report(results['performance_metrics'])
